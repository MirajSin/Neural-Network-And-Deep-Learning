{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIeXlBMHTJZyKFfmX09AR+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AMl 3104 Decision Tree Assignment\n",
        "Student Name: Miraj Sinya\n",
        "\n",
        "Student ID: c0863371\n",
        "\n",
        "Github repo link: https://github.com/MirajSin/Neural-Network-And-Deep-Learning"
      ],
      "metadata": {
        "id": "zohNT11dTHd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Describe the decision tree classifier algorithm and how it works to make predictions.**"
      ],
      "metadata": {
        "id": "mSoIAnzIwxTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a hierarchical structure that resembles an inverted tree, with a single root node at the top and various internal nodes and leaf nodes where each node represents a decision or a test on a feature in the dataset. The decision tree classifier algorithm works by recursively splitting the data into subsets based on feature values and thresholds, leading to a leaf node that provides the prediction.\n",
        "\n",
        "The algorithm selects features and thresholds to split the data at each internal node in a way that minimizes impurity or error, such as Gini impurity (for classification) or mean squared error (for regression). This process continues recursively until a stopping condition is met, such as a maximum tree depth or a minimum number of data points in a leaf node. Decision tree classifier works on making predictions by:\n",
        "\n",
        "1. Node Splitting:\n",
        "\n",
        "At each internal node, the algorithm selects a feature from the dataset and a corresponding threshold value. This feature and threshold are chosen to split the data into two or more subsets in such a way that they maximize the separation between different classes in the target variable (for classification tasks) or minimize the variance (for regression tasks).\n",
        "\n",
        "2. Decision Making:\n",
        "\n",
        "When a new data point needs to be classified or predicted, it is passed down the tree starting from the root node. At each internal node, the algorithm compares the feature value of the data point with the threshold. Depending on the outcome, the algorithm follows the corresponding branch of the tree to the next internal node. This process continues until a leaf node is reached.\n",
        "\n",
        "3. Leaf Nodes and Predictions:\n",
        "\n",
        "Each leaf node in the decision tree represents a class label (in classification) or a numerical value (in regression). When the algorithm reaches a leaf node, it assigns the class label or the numerical value associated with that leaf node as the prediction for the input data point.\n",
        "\n",
        "4. Pruning:\n",
        "\n",
        "Decision trees can be prone to overfitting, where they capture noise in the training data. Pruning is a technique used to reduce the complexity of the tree by removing some branches, which can improve the tree's generalization performance on unseen data."
      ],
      "metadata": {
        "id": "7QGtW8AKxcGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QE0KBugvLFf",
        "outputId": "8f29baa5-32d5-4206-d297-31214ebb1ee3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.**"
      ],
      "metadata": {
        "id": "6v8tvvz3w-C6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree classification is a mathematical process that involves measuring impurity, selecting the best features and thresholds to split the data, and building a tree structure to make predictions. The choice of features and thresholds is guided by criteria such as Gini impurity and information gain, which aim to reduce impurity and create informative splits in the data. The mathemaatical intuition behind the decision tree classification can be explained with the following concepts:\n",
        "\n",
        "1. Impurity Measure:\n",
        "\n",
        "At the heart of decision tree classification is the concept of impurity. Impurity is a measure of how mixed the class labels are in a subset of data. Common impurity measures include Gini impurity and entropy.\n",
        "  \n",
        "  1.1. Gini Impurity:\n",
        "\n",
        "Gini impurity measures the probability of misclassifying a randomly chosen element from the dataset. It is mathematically defined as:\n",
        "Gini Impurity = 1 - Σ (p_i)^2\n",
        "\n",
        "where p_i is the proportion of instances belonging to class i in the subset.\n",
        "\n",
        "A Gini impurity of 0 indicates that all elements in the subset belong to the same class, while an impurity of 0.5 indicates a perfectly mixed dataset with equal representation of all classes.\n",
        "\n",
        "  1.2. Entropy:\n",
        "\n",
        "Entropy measures the level of disorder or uncertainty in a dataset. It is defined as:\n",
        "Entropy = - Σ p_i * log2(p_i)\n",
        "\n",
        "Similar to Gini impurity, entropy is 0 when the data is pure (all elements belong to the same class) and higher when the data is mixed.\n",
        "\n",
        "2. Splitting Criteria:\n",
        "\n",
        "The goal of a decision tree is to create splits in the data that minimize impurity. This means finding a feature and a threshold value that separates the data into subsets with the least impurity.\n",
        "\n",
        "3. Information Gain:\n",
        "\n",
        "Information gain is a measure of how much the impurity in a dataset is reduced after a particular split. It is calculated as the impurity of the parent node minus the weighted average impurity of the child nodes after the split. The feature and threshold that result in the highest information gain are chosen for the split."
      ],
      "metadata": {
        "id": "xQsExoc1zXi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "# Function to calculate Gini impurity\n",
        "def gini_impurity(labels):\n",
        "    total_samples = len(labels)\n",
        "    if total_samples == 0:\n",
        "        return 0\n",
        "\n",
        "    unique_classes = np.unique(labels)\n",
        "    gini = 1\n",
        "\n",
        "    for c in unique_classes:\n",
        "        p_c = np.sum(labels == c) / total_samples\n",
        "        gini -= p_c ** 2\n",
        "\n",
        "    return gini\n",
        "\n",
        "# Calculate Gini impurity for the entire dataset\n",
        "gini_initial = gini_impurity(Y)\n",
        "\n",
        "print(f\"Initial Gini Impurity: {gini_initial:.2f}\")\n",
        "\n",
        "# Function to calculate entropy\n",
        "def entropy(labels):\n",
        "    total_samples = len(labels)\n",
        "    if total_samples == 0:\n",
        "        return 0\n",
        "\n",
        "    entropy_value = 0\n",
        "    unique_classes = np.unique(labels)\n",
        "\n",
        "    for c in unique_classes:\n",
        "        p_c = np.sum(labels == c) / total_samples\n",
        "        if p_c > 0:\n",
        "            entropy_value -= p_c * np.log2(p_c)\n",
        "\n",
        "    return entropy_value\n",
        "\n",
        "# Calculate entropy for the entire dataset\n",
        "entropy_initial = entropy(Y)\n",
        "\n",
        "print(f\"Initial Entropy: {entropy_initial:.2f}\")\n",
        "\n",
        "# Splitting the dataset into two subsets based on a selected feature and threshold\n",
        "feature_index = 3  # Example feature index (petal width in this case)\n",
        "threshold = 1.0  # Example threshold\n",
        "\n",
        "left_subset_indices = [i for i, x in enumerate(X[:, feature_index]) if x < threshold]\n",
        "right_subset_indices = [i for i, x in enumerate(X[:, feature_index]) if x >= threshold]\n",
        "\n",
        "# Calculate Gini impurity and entropy for the left subset\n",
        "left_Y = Y[left_subset_indices]\n",
        "gini_left = gini_impurity(left_Y)\n",
        "entropy_left = entropy(left_Y)\n",
        "\n",
        "# Calculate Gini impurity and entropy for the right subset\n",
        "right_Y = Y[right_subset_indices]\n",
        "gini_right = gini_impurity(right_Y)\n",
        "entropy_right = entropy(right_Y)\n",
        "\n",
        "# Calculate the weighted average of Gini impurity and entropy after the split\n",
        "total_samples = len(Y)\n",
        "weighted_gini = (len(left_Y) / total_samples) * gini_left + (len(right_Y) / total_samples) * gini_right\n",
        "weighted_entropy = (len(left_Y) / total_samples) * entropy_left + (len(right_Y) / total_samples) * entropy_right\n",
        "\n",
        "# Calculate information gain\n",
        "information_gain_gini = gini_initial - weighted_gini\n",
        "information_gain_entropy = entropy_initial - weighted_entropy\n",
        "\n",
        "print(f\"Gini Impurity after Split: {weighted_gini:.2f}\")\n",
        "print(f\"Information Gain (Gini): {information_gain_gini:.2f}\")\n",
        "\n",
        "print(f\"Entropy after Split: {weighted_entropy:.2f}\")\n",
        "print(f\"Information Gain (Entropy): {information_gain_entropy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9LyiirgxYdM",
        "outputId": "cfa5b3c3-6fd6-4052-bc85-44a365e031a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Gini Impurity: 0.67\n",
            "Initial Entropy: 1.58\n",
            "Gini Impurity after Split: 0.33\n",
            "Information Gain (Gini): 0.33\n",
            "Entropy after Split: 0.67\n",
            "Information Gain (Entropy): 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information gain numbers show that there is significant reduction in entrophy with a values of 0.92 after the split than compared to gini impurity with a value of 0.33."
      ],
      "metadata": {
        "id": "fYD91uLe4MZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.**\n"
      ],
      "metadata": {
        "id": "GXbJJ4DxxCSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree classifier can be used to solve a binary classification problem, where the goal is to classify data into one of two classes. Decision tree classifier for binary classification involves selecting features, building the tree by splitting the data to minimize impurity, making predictions by traversing the tree, and evaluating the model's performance.  Solving binary classification problem using decision tree classifer involves the following steps:\n",
        "\n",
        "1. Data Preparation:\n",
        "\n",
        "Start with a dataset that is labeled with two classes, typically denoted as Class A and Class B. Each data point in the dataset should have features (attributes) and a corresponding class label.\n",
        "2. Feature Selection:\n",
        "\n",
        "Choose the features from the dataset that will be used to make the classification decision. The selection of features is critical because it influences the effectiveness of the decision tree.\n",
        "3. Building the Decision Tree:\n",
        "\n",
        "The process of building the decision tree involves selecting the best features and thresholds to split the data into subsets that minimize impurity (e.g., Gini impurity or entropy). The following steps are involved:\n",
        "\n",
        "3.1. Calculate the impurity of the entire dataset based on the class labels. This is the impurity of the root node.\n",
        "\n",
        "3.2. For each feature, calculate the information gain (or Gini gain) by splitting the data based on that feature's values. Information gain is the difference between the impurity of the current node and the weighted average impurity of the child nodes after the split.\n",
        "\n",
        "3.3. Choose the feature that maximizes information gain or minimizes impurity and select the corresponding threshold value for the split.\n",
        "\n",
        "3.4. Split the data into two subsets based on the selected feature and threshold, creating two child nodes.\n",
        "\n",
        "3.5. Repeat the process recursively for each child node, choosing the best feature and threshold for further splits.\n",
        "\n",
        "3.6. Continue this process until a stopping condition is met, such as reaching a maximum tree depth or having a minimum number of data points in a leaf node.\n",
        "\n",
        "4. Prediction:\n",
        "\n",
        "To make predictions for new, unseen data points, we traverse the decision tree. Starting from the root node, you compare the feature values of the data point with the threshold values at each internal node and follow the appropriate branch until a leaf node is reached.\n",
        "\n",
        "The class label associated with the reached leaf node is the prediction for the binary classification problem. For example, if the leaf node corresponds to Class A, we predict that the data point belongs to Class A; otherwise, we predict Class B.\n",
        "\n",
        "5. Evaluation:\n",
        "\n",
        "After building the decision tree and making predictions on a test dataset, we evaluate the model's performance using appropriate evaluation metrics such as accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC), depending on your problem and objectives.\n"
      ],
      "metadata": {
        "id": "vjrpZbMK11RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "df = load_iris()\n",
        "\n",
        "data = pd.DataFrame(df.data, columns = df.feature_names)\n",
        "data['target'] = df.target\n",
        "final_data = data[data['target']!=2] #Creting a binary problem from the iris dataset with the exclusion of class 2 from the targets\n",
        "\n",
        "#Train test split to train the model\n",
        "X = final_data.drop('target', axis = 1)\n",
        "y = final_data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk0OgMyFQkWD",
        "outputId": "9d5998d0-c6f4-4300-ae0f-785c5f3ce5cf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            "[[12  0]\n",
            " [ 0  8]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
        "predictions.**\n"
      ],
      "metadata": {
        "id": "qhg0ab0VxEd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The geometric intuition behind decision tree classification is closely related to the idea of partitioning the feature space into regions, with each region corresponding to a particular class. Decision tree classification aims to find a set of rules (splits) in feature space that best separates the different classes, creating these partitions.\n",
        "\n",
        "The geometric intuition behind decision tree classification is that it divides the feature space into regions using decision boundaries (hyperplanes) defined by features and thresholds. These regions correspond to different classes, and predictions are made by assigning data points to the region in which they fall. This approach provides an intuitive way to understand how a decision tree algorithm works and why it makes certain predictions based on the input feature values. Makng predictions using geometric intuition involves:\n",
        "\n",
        "1. Feature Space Partitioning:\n",
        "\n",
        "Imagine the feature space as a multi-dimensional space where each axis represents a feature or attribute of the data. In binary classification, there are two classes, typically Class A and Class B. The goal is to divide this feature space into regions or subspaces such that each region is predominantly associated with one of the two classes.\n",
        "2. Decision Boundaries:\n",
        "\n",
        "Decision tree classification identifies decision boundaries in the feature space. These decision boundaries are typically orthogonal to the feature axes (for simplicity), but they can be oblique as well. Each decision boundary is associated with a node in the tree, and it separates data points belonging to different classes.\n",
        "3. Splitting Nodes:\n",
        "\n",
        "When building a decision tree, we choose features and thresholds for splitting nodes. The selected feature corresponds to one of the feature axes, and the threshold corresponds to a position along that axis.\n",
        "4. Geometric Split:\n",
        "\n",
        "Consider a single split in the decision tree. It is like drawing a hyperplane (in the case of 2D, it's just a line) across the feature space that partitions the data. If the feature values of a data point fall on one side of the hyperplane, it is assigned to one class; if on the other side, it is assigned to the other class.\n",
        "5. Recursive Partitioning:\n",
        "\n",
        "The process is repeated recursively for each child node, creating a hierarchical structure that further divides the feature space. Each split or node in the tree results in a new hyperplane, further refining the partitioning.\n",
        "6. Making Predictions:\n",
        "\n",
        "To make predictions for a new data point, we start at the root node of the tree and traverse the tree. At each internal node, we compare the feature values of the data point with the threshold of that node.\n",
        "\n",
        "Depending on the outcome of the comparison, we move to the left or right child node, following the appropriate branch. This process continues until a leaf node is reached, which represents the region in the feature space where the data point belongs.\n",
        "\n",
        "The class label associated with that leaf node is the prediction for the data point. If we are performing binary classification, we will assign it to Class A or Class B based on the class label of the leaf node.\n",
        "\n",
        "7. Visualizing Decision Trees:\n",
        " We can visualize decision trees in 2D by drawing the decision boundaries as lines, and in higher dimensions as hyperplanes. These visualizations help us understand how the feature space is partitioned and why specific predictions are made.\n",
        "\n"
      ],
      "metadata": {
        "id": "z5vBK9Qd2ZhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
        "classification model.**\n"
      ],
      "metadata": {
        "id": "zj_KTnWnxG0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table used in the field of machine learning and statistics to evaluate the performance of a classification model. It provides a clear and comprehensive summary of the model's predictions compared to the actual ground truth, that helps understand the model's accuracy, precision, recall, and other performance metrics. A confusion matrix is typically used in binary classification, where there are two classes, but it can be adapted for multi-class classification as well. It is often used in the context of evaluating the performance of classification models such as decision trees, support vector machines, logistic regression, and more.\n",
        "\n",
        "A standard confusion matrix consists of four main components:\n",
        "\n",
        "- True Positives (TP): The number of instances that were correctly predicted as positive (belonging to the positive class).\n",
        "\n",
        "- True Negatives (TN): The number of instances that were correctly predicted as negative (belonging to the negative class).\n",
        "\n",
        "- False Positives (FP): The number of instances that were incorrectly predicted as positive when they were actually negative. This is also known as a Type I error.\n",
        "\n",
        "- False Negatives (FN): The number of instances that were incorrectly predicted as negative when they were actually positive. This is also known as a Type II error.\n",
        "\n",
        "The confusion matrix can be use to calculate the following metrices to evaluate the performance of a classification model:\n",
        "1. Accuracy:\n",
        "\n",
        "Accuracy is a common performance metric and is calculated as (TP + TN) / (TP + TN + FP + FN). It measures the overall correctness of predictions and is the ratio of correctly classified instances to the total number of instances.\n",
        "2. Precision (Positive Predictive Value):\n",
        "\n",
        "Precision is calculated as TP / (TP + FP). It measures the proportion of true positive predictions among all positive predictions that shows how many of the predicted positive instances were actually positive.\n",
        "3. Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Recall is calculated as TP / (TP + FN). It measures the ability of the model to correctly identify all actual positive instances that tells how many of the actual positive instances were correctly predicted.\n",
        "4. Specificity (True Negative Rate):\n",
        "\n",
        "Specificity is calculated as TN / (TN + FP). It measures the ability of the model to correctly identify all actual negative instances.\n",
        "5. F1 Score:\n",
        "\n",
        "The F1 Score is the harmonic mean of precision and recall and is calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balance between precision and recall.\n",
        "6. Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC):\n",
        "\n",
        "The ROC curve is a graphical representation of the model's performance across different thresholds. The AUC measures the area under the ROC curve, which quantifies the model's ability to distinguish between the positive and negative classes.\n",
        "7. Evaluation of Class Imbalance:\n",
        "\n",
        "In cases where the classes are imbalanced (one class has significantly more instances than the other), the confusion matrix helps understand if the model is biased toward the majority class. It's important to consider precision and recall for each class in such scenarios.\n"
      ],
      "metadata": {
        "id": "wKs7ia3V20wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Iris dataset\n",
        "df = load_iris()\n",
        "\n",
        "data = pd.DataFrame(df.data, columns = df.feature_names)\n",
        "data['target'] = df.target\n",
        "final_data = data[data['target']!=2] #Creting a binary problem from the iris dataset with the exclusion of class 2 from the targets\n",
        "\n",
        "#Train test split to train the model\n",
        "X = final_data.drop('target', axis = 1)\n",
        "y = final_data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "y_prob = clf.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "auc_roc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(f\"AUC-ROC Score: {auc_roc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "2j7BXXe_S5xK",
        "outputId": "f8a05ca7-b66c-4d5a-bdc2-2dc6b0b6ee26"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "AUC-ROC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
        "calculated from it.**\n"
      ],
      "metadata": {
        "id": "TMOhQjXfxJlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " |                 | Actual Positive | Actual Negative |\n",
        "  |-----------------|-----------------|-----------------|\n",
        "  | Predicted Positive |85            | 15            |\n",
        "  | Predicted Negative | 10            | 90            |\n",
        "\n",
        "In this confusion matrix:\n",
        "\n",
        "Actual Positive (AP) represents the number of individuals who actually have the disease.\n",
        "Actual Negative (AN) represents the number of individuals who do not have the disease.\n",
        "Predicted Positive (PP) represents the number of individuals predicted to have the disease.\n",
        "Predicted Negative (PN) represents the number of individuals predicted not to have the disease.\n",
        "Now, let's calculate precision, recall, and F1 score using these values:\n",
        "\n",
        "1. Precision:\n",
        "\n",
        "Precision is the proportion of true positive predictions among all positive predictions. It is calculated as:\n",
        "\n",
        "Precision = TP / (TP + FP) = 85 / (85 + 15) = 85 / 100 = 0.85\n",
        "So, the precision is 0.85, meaning that 85% of the predicted positive cases were correct.\n",
        "\n",
        "2. Recall:\n",
        "\n",
        "Recall is the proportion of true positive predictions among all actual positive instances. It is calculated as:\n",
        "\n",
        "Recall = TP / (TP + FN) = 85 / (85 + 10) = 85 / 95 ≈ 0.8947\n",
        "So, the recall is approximately 0.8947, meaning that the model can correctly identify about 89.47% of the actual positive cases.\n",
        "\n",
        "3. F1 Score:\n",
        "\n",
        "The F1 score is the harmonic mean of precision and recall and provides a balance between these two metrics. It is calculated as:\n",
        "\n",
        "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "         = 2 * (0.85 * 0.8947) / (0.85 + 0.8947)\n",
        "         ≈ 0.8725\n",
        "The F1 score is approximately 0.8725, representing a balance between precision and recall. It indicates the overall performance of the model in terms of both false positives and false negatives.\n",
        "\n"
      ],
      "metadata": {
        "id": "LVi_cYmY6WDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
        "explain how this can be done.**\n"
      ],
      "metadata": {
        "id": "ragn_06QxLi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing an appropriate evaluation metric for a classification problem is crucial because it directly impacts how we assess the performance of our model and make informed decisions. Different evaluation metrics highlight different aspects of a model's performance, and the choice of metric should align with our specific problem, goals, and the relative costs associated with different types of errors. Some of the reasons why choosing an appropriate evaluation metric for classification problem are:\n",
        "\n",
        "1. Understanding the Problem:\n",
        "\n",
        "Different classification problems have different requirements. For example, in a medical diagnosis task, we might want to minimize false negatives (missing a disease diagnosis), while in a spam email detection task, we may be more concerned about false positives (marking a legitimate email as spam). The choice of metric should align with these considerations.\n",
        "2. Balancing Trade-offs:\n",
        "\n",
        "Metrics like precision, recall, and the F1 score provide a balance between different types of errors. Precision is concerned with minimizing false positives, while recall focuses on minimizing false negatives. The F1 score combines these two aspects. Depending on the importance of each error type, we can choose the metric that best reflects our priorities.\n",
        "3. Imbalanced Data:\n",
        "\n",
        "In cases where one class significantly outnumbers the other (class imbalance), accuracy may not be an appropriate metric because it can be misleading. Metrics like precision, recall, or the area under the ROC curve (AUC) may be more appropriate to provide a more accurate representation of the model's performance.\n",
        "\n",
        "4. Model Interpretability:\n",
        "\n",
        "Some metrics are more interpretable and can provide insights into the model's behavior. For example, the confusion matrix allows us to see how well the model performs in different categories, which can help identify areas for improvement.\n",
        "5. Comparative Analysis:\n",
        "\n",
        "When comparing multiple models or approaches, having a consistent evaluation metric is essential. It allows us to assess which model performs better for your specific problem.\n",
        "\n",
        "Choosing the appropriate evaluation metric is essential for assessing the performance of a classification model in a way that aligns with the problem's specific requirements and constraints. It involves a thoughtful analysis of the problem, the relative importance of different errors, and the impact of the model's predictions.There are certain ways we can choose an evlauation metric. Some of which are:\n",
        "\n",
        "1. Understand the Problem and Objectives: First, understanding the nature of the classification problem and the specific goals and constraints is important taking into consideration the business context and the consequences of different types of errors.\n",
        "\n",
        "2. Consult Domain Experts: Collaborate with domain experts or stakeholders to gain insights into the relative importance of true positives, true negatives, false positives, and false negatives in your application.\n",
        "\n",
        "3. Use Multiple Metrics: It's often a good practice to report multiple metrics, including accuracy, precision, recall, F1 score, and AUC. This provides a more comprehensive view of your model's performance.\n",
        "\n",
        "4. Adjust Thresholds: In some cases, the classification threshold can be adjusted to achieve a better balance between precision and recall. By changing the threshold, we can control the trade-off between different error types.\n",
        "\n",
        "5. Iterate and Refine: Continuously monitoring and evaluating the model's performance as we gather more data and adapt to changing conditions. and making adjustments to our choice of evaluation metric as needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "wqKfISPa8gFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
        "explain why.**\n"
      ],
      "metadata": {
        "id": "DS4CmEzqxNhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit card fraud detection is a good example of a classification problem where precision is the most importatnt metric.In credit card fraud detection, the goal is to identify and prevent fraudulent credit card transactions. It's crucial to catch as many fraudulent transactions as possible (high recall) while minimizing false alarms (high precision).\n",
        "\n",
        "Precision is important in this scenario because making a false positive prediction, i.e., incorrectly flagging a legitimate transaction as fraudulent, can have significant consequences. Here's why precision matters the most in this case:\n",
        "\n",
        "1. Fraud Detection Capabilities: If the system triggers false alarms frequently, it may lead to complacency in the fraud detection team. They might start ignoring alerts, assuming they are likely to be false positives, which could result in more fraudulent transactions going undetected.\n",
        "\n",
        "2. Customer Experience: False positives can result in legitimate transactions being declined. This can be highly frustrating for customers and may lead to a poor user experience. It might even lead to customers changing their payment methods or abandoning the service.\n",
        "\n",
        "3. Reputation and Trust: False positives can erode trust between the bank or credit card company and its customers. Repeated false alarms can lead customers to believe that the institution is not safeguarding their interests effectively.\n",
        "\n",
        "4. Operational Costs: Investigating and resolving false positives can be costly for the credit card company. It consumes resources, time, and effort, which could be better spent elsewhere.\n"
      ],
      "metadata": {
        "id": "erguYi-48ytp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Provide an example of a classification problem where recall is the most important metric and explain\n",
        "why.**"
      ],
      "metadata": {
        "id": "56IyewHcxPht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a classification problem where recall is the most important metric, cancer detection can be a good examle. In early cancer detection, the goal is to identify individuals who might have cancer at an early stage. The focus is on maximizing the detection of true positive cases (high recall) while minimizing the risk of missing actual cancer cases (low false negatives).Here are some reacons why recall is the most important metric in this case:\n",
        "\n",
        "1. Reducing False Negatives: False negatives, where the model fails to detect actual cancer cases, can lead to severe consequences, both for the individual's health and for healthcare providers.\n",
        "\n",
        "2. Early Intervention: Detecting cancer at an early stage significantly improves the chances of successful treatment and recovery. Missing a cancer case (false negative) could delay diagnosis, resulting in more advanced and less treatable stages of the disease. Therefore, the primary concern is capturing as many true positives as possible to enable early intervention.\n",
        "\n",
        "3. Diagnostic Sensitivity: Recall in the context of healthcare is often referred to as \"sensitivity.\" A highly sensitive test ensures that it detects as many true positive cases as possible, making it a crucial measure for medical screening and early disease detection.\n"
      ],
      "metadata": {
        "id": "p2ZyRx6t8-7f"
      }
    }
  ]
}